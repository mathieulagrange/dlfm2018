
The expressive variability in which a musical note can be produced conveys essential information to the modeling of style and orchestration.
Yet, although the automatic recognition of a musical instrument from the recording of a single "ordinary" note is now considered a solved problem, the ability of a computer to precisely identify instrumental playing techniques within a large taxonomy remains far below human accuracy.
This article provides the first benchmark of machine listening systems for query-by-example browsing of instrumental playing techniques, including "extended" techniques such as reed slaps and , in the symphonic orchestra.
We identify three necessary conditions for significantly outperforming the classical mel-frequency cepstral coefficients (MFCC) baseline:
(1) the inclusion of second-order scattering coefficients to account for the presence of amplitude modulations ;
(2) the inclusion of large temporal scales ; and
(3) the resort to supervised metric learning.